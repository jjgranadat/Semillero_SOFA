{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b52bd51d-80dd-427d-a28f-1ca93b6321b2"
   },
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRvl6KqttJ5y",
    "outputId": "e8a4cbfe-c017-462c-e4ff-571ce7e772a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras==2.9 in /home/jupyter-santiago.vargas5/.local/lib/python3.10/site-packages (2.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "twmEuSk04RrX",
    "outputId": "1ebe3f5f-e2d0-43d1-f14a-d8e0288904bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow==2.9 in /home/jupyter-santiago.vargas5/.local/lib/python3.10/site-packages (2.9.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /home/jupyter-santiago.vargas5/.local/lib/python3.10/site-packages (from tensorflow==2.9) (1.12)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/jupyter-santiago.vargas5/.local/lib/python3.10/site-packages (from tensorflow==2.9) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (1.60.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (3.10.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/jupyter-santiago.vargas5/.local/lib/python3.10/site-packages (from tensorflow==2.9) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/jupyter-santiago.vargas5/.local/lib/python3.10/site-packages (from tensorflow==2.9) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (16.0.6)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (1.26.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (23.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/jupyter-santiago.vargas5/.local/lib/python3.10/site-packages (from tensorflow==2.9) (3.19.6)\n",
      "Requirement already satisfied: setuptools in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (65.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /home/jupyter-santiago.vargas5/.local/lib/python3.10/site-packages (from tensorflow==2.9) (2.9.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (0.35.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/jupyter-santiago.vargas5/.local/lib/python3.10/site-packages (from tensorflow==2.9) (2.9.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/tljh/user/lib/python3.10/site-packages (from tensorflow==2.9) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/tljh/user/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.9) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/tljh/user/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (2.25.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/jupyter-santiago.vargas5/.local/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/tljh/user/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/tljh/user/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/jupyter-santiago.vargas5/.local/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/jupyter-santiago.vargas5/.local/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/tljh/user/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/tljh/user/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/tljh/user/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/tljh/user/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/tljh/user/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/tljh/user/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/tljh/user/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/tljh/user/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCZszctz5ji3",
    "outputId": "2f092f76-80da-4841-c7f0-1f7a079a46ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 21:25:36.896333: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-01-29 21:25:36.896425: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5bae1c36-6bd6-42c6-b306-7b59124b0567"
   },
   "outputs": [],
   "source": [
    "import sofa\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "# SciPy\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow.keras import models, regularizers, utils\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQ7mBLsT55mI"
   },
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uMeX68Zaxx-"
   },
   "source": [
    "## Demodulation Single Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demodulation_single(X_rx, X_tx, spacing,lista_neuronas, OSNR,n_splits,activations, database):\n",
    "    \"\"\"\n",
    "    Demodulación para single channel\n",
    "\n",
    "    :param X_rx: Diccionario con los datos recibidos (por espaciamiento)\n",
    "    :param X_tx: Datos recibidos (Se lee previamente)\n",
    "    :spacing: Espaciamiento actual (para escribir en archivo)\n",
    "    :OSNR: Lista con los valores de OSNR para calcular su respectivo BER\n",
    "    :n_splits: Numero de kFolds para la red\n",
    "    :activations: Funciones de Activación para las capas de la red\n",
    "    \"\"\"\n",
    "    folder = \"/home/jupyter-santiago.vargas5/DNN_Demodulation/results\"\n",
    "    #Variar el OSNR\n",
    "    for i, snr in enumerate(X_rx):#Cambiar OSNR por X_rx si se desea calcular con todos los valores\n",
    "        #snr=str(snr)+\"dB\"\n",
    "        # Extraer información\n",
    "        if snr not in database[\"single_ch\"].keys():\n",
    "            database[\"single_ch\"][snr] = {}\n",
    "            print(f\"=================Iniciando {snr}=================\")\n",
    "        else:\n",
    "            print(f\"=================Continuando {snr}=================\")\n",
    "        #X_ch_norm = X_rx[snr].get(\"const_Y\").flatten()\n",
    "        X_ch_norm = np.array(X_rx[snr]['I'])+np.array(X_rx[snr]['Q'])*1j\n",
    "        X_ch = sofa.mod_norm(X_ch_norm, 10) * X_ch_norm\n",
    "\n",
    "        for neuron in lista_neuronas:\n",
    "            if  (str(neuron)) not in database[\"single_ch\"][snr].keys():\n",
    "                DNN_BER_lista = []\n",
    "                print(f\"***********RED NEURONAL***********\\n{neuron} Neuronas\")\n",
    "                # Sincronizar de las señales\n",
    "                synced_X_tx = sofa.sync_signals(X_tx,  X_ch)[0]\n",
    "                # Demodular señal transmitida\n",
    "                y = sofa.demodulate(synced_X_tx, sofa.MOD_DICT)\n",
    "\n",
    "                #Inicio asignacion de parametros para la red\n",
    "                max_neurons = neuron\n",
    "                layer_props_lst = [\n",
    "                    {\"units\": max_neurons // (2**i), \"activation\": activation}\n",
    "                    for i, activation in enumerate(activations)\n",
    "                ]\n",
    "                loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "                # Demodulación para redes\n",
    "                t_inic = time.time()\n",
    "                dem_DNN = sofa.demodulate_neural(\n",
    "                    X_ch,\n",
    "                    y,\n",
    "                    layer_props_lst=layer_props_lst,\n",
    "                    loss_fn=loss_fn,\n",
    "                    n_splits = n_splits,\n",
    "                    )\n",
    "                demodulated, tests = dem_DNN\n",
    "                #Calcular BER para cada KFold\n",
    "                for demod, test in zip(demodulated, tests):\n",
    "                    x_ch_dnn = np.argmax(demod, axis = 1)\n",
    "                    k_BER = sofa.bit_error_rate(x_ch_dnn, test)\n",
    "                    DNN_BER_lista.append(k_BER)\n",
    "                print(f\"Lista BER DNN: {DNN_BER_lista}\")\n",
    "                DNN_BER = np.mean(np.array(DNN_BER_lista))\n",
    "                print(f\"DNN mean BER=> {DNN_BER}\")\n",
    "                t_fin = time.time()\n",
    "                print(f\"Tiempo DNN: {t_fin - t_inic}\\n\")\n",
    "\n",
    "                database[\"single_ch\"][snr][neuron] = DNN_BER\n",
    "                sofa.save_json(database, folder)\n",
    "                print(f\"Calculo con {neuron} NEURONAS terminado para OSNR {snr}dB \")\n",
    "        print(\"Se han recorrido todas las neuronas\")\n",
    "    return database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_hIf5rLa3IP"
   },
   "source": [
    "## Demodulation General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Z78RVD23PVNU"
   },
   "outputs": [],
   "source": [
    "def demodulation(X_rx, X_tx, spacing,lista_neuronas, OSNR,n_splits,activations, database, especific):\n",
    "    \"\"\"\n",
    "    Demodulación con DNN para los casos que no son single channel\n",
    "\n",
    "    :param X_rx: Diccionario con los datos recibidos (por espaciamiento)\n",
    "    :param X_tx: Datos recibidos (Se lee previamente)\n",
    "    :spacing: Espaciamiento actual (para escribir en archivo)\n",
    "    :OSNR: Lista con los valores de OSNR para calcular su respectivo BER\n",
    "    :n_splits: Numero de kFolds para la red\n",
    "    :activations: Funciones de Activación para las capas de la red\n",
    "    :database: Resultados en la base de datos\n",
    "    :especific: Seleccionar snr específicos de la lista OSNR (true). Seleccionar todos\n",
    "    \"\"\"\n",
    "    l_OSNR = X_rx\n",
    "    if especific:\n",
    "      l_OSNR = OSNR\n",
    "\n",
    "    folder = \"/home/jupyter-santiago.vargas5/DNN_Demodulation/results\"\n",
    "    #Variar el OSNR\n",
    "    for i, snr in enumerate(l_OSNR):#Cambiar OSNR por X_rx si se desea calcular con todos los valores\n",
    "        # Extraer información\n",
    "        if especific: #En database los datos vienen con una etiqueta específica\n",
    "          snr=str(snr)+\"dB\"\n",
    "        print(snr)\n",
    "        if snr not in database[f\"{spacing}GHz\"].keys():\n",
    "            database[f\"{spacing}GHz\"][snr] = {}\n",
    "            print(f\"=================Iniciando {snr}=================\")\n",
    "        else:\n",
    "            print(f\"=================Continuando {snr}=================\")\n",
    "\n",
    "        X_ch_norm = np.array(X_rx[snr]['I'])+np.array(X_rx[snr]['Q'])*1j\n",
    "        X_ch = sofa.mod_norm(X_ch_norm, 10) * X_ch_norm\n",
    "        sufix=\"\"\n",
    "        for act in activations:#Etiquetar por funcion de activación\n",
    "          sufix=sufix+act[0]\n",
    "        print(sufix)\n",
    "        for neuron in lista_neuronas:\n",
    "            if  (sufix+str(neuron)) not in database[f\"{spacing}GHz\"][snr].keys():\n",
    "                DNN_BER_lista = []\n",
    "                print(f\"***********RED NEURONAL***********\\n{neuron} Neuronas\")\n",
    "                # Sincronizar de las señales\n",
    "                synced_X_tx = sofa.sync_signals(X_tx,  X_ch)[0]\n",
    "                # Demodular señal transmitida\n",
    "                y = sofa.demodulate(synced_X_tx, sofa.MOD_DICT)\n",
    "\n",
    "                #Inicio asignacion de parametros para la red\n",
    "                max_neurons = neuron\n",
    "                layer_props_lst = [\n",
    "                    {\"units\": max_neurons // (2**i), \"activation\": activation}\n",
    "                    for i, activation in enumerate(activations)\n",
    "                ]\n",
    "                #Función de pérdida\n",
    "                loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "                t_inic = time.time()#Medir el tiempo\n",
    "                dem_DNN = sofa.demodulate_neural(\n",
    "                    X_ch,\n",
    "                    y,\n",
    "                    layer_props_lst=layer_props_lst,\n",
    "                    loss_fn=loss_fn,\n",
    "                    n_splits = n_splits,\n",
    "                    )\n",
    "                demodulated, tests = dem_DNN\n",
    "                #Calcular BER para cada KFold\n",
    "                for demod, test in zip(demodulated, tests):\n",
    "                    x_ch_dnn = np.argmax(demod, axis = 1)\n",
    "                    k_BER = sofa.bit_error_rate(x_ch_dnn, test)\n",
    "                    DNN_BER_lista.append(k_BER)\n",
    "\n",
    "                print(f\"Lista BER DNN: {DNN_BER_lista}\")\n",
    "                #Promedio del los BER encontrados\n",
    "                DNN_BER = np.mean(np.array(DNN_BER_lista))\n",
    "                print(f\"DNN mean BER=> {DNN_BER}\")\n",
    "                t_fin = time.time()\n",
    "                print(f\"Tiempo DNN: {t_fin - t_inic}\\n\")\n",
    "\n",
    "                database[f\"{spacing}GHz\"][snr][f\"{sufix}{neuron}\"] = DNN_BER\n",
    "                sofa.save_json(database, folder)\n",
    "                print(f\"Calculo con {neuron} NEURONAS terminado para OSNR {snr}\")\n",
    "        print(\"Se han recorrido todas las neuronas\")\n",
    "    return database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEUURa50YqQ3"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3d7e4612-ad50-4dd6-8d3b-4d69c1e839e4"
   },
   "outputs": [],
   "source": [
    "# Función especial para leer todos los datos con la estructura estudiada\n",
    "def read_data(folder_rx):\n",
    "    data = {}\n",
    "\n",
    "    # Leer la carpeta principal\n",
    "    for folder in os.listdir(folder_rx):\n",
    "        # Leer las subcarpetas\n",
    "        if folder.endswith(\"spacing\"):\n",
    "            data[folder] = {}\n",
    "            for file in os.listdir(f\"{folder_rx}/{folder}\"):\n",
    "                if file.find(\"consY\") != -1:\n",
    "                    data_name = file.split(\"_\")[2]\n",
    "                    if data[folder].get(data_name) == None:\n",
    "                        data[folder][data_name] = {}\n",
    "                    mat_file_data = loadmat(f\"{folder_rx}/{folder}/{file}\")\n",
    "                    data[folder][data_name] = mat_file_data\n",
    "    return data\n",
    "# Special function to read the known data structure\n",
    "def read_data(folder_rx, ends):\n",
    "    data = {}\n",
    "\n",
    "    # Read root directory\n",
    "    for folder in os.listdir(folder_rx):\n",
    "        # Check name consistency for subdirectories\n",
    "        if folder.endswith(ends):\n",
    "            # Extract \"pretty\" part of the name\n",
    "            spacing = folder[:-8]\n",
    "            data[spacing] = {}\n",
    "\n",
    "            # Read each data file\n",
    "            for file in os.listdir(f\"{folder_rx}/{folder}\"):\n",
    "                # Check name consistency for data files\n",
    "                if file.find(\"consY\") != -1:\n",
    "                    # Extract \"pretty\" part of the name\n",
    "                    osnr = file.split(\"_\")[2][5:-4]\n",
    "\n",
    "                    # Initialize if not created yet\n",
    "                    if data[spacing].get(osnr) == None:\n",
    "                        data[spacing][osnr] = {}\n",
    "                    # Set data\n",
    "                    csv_file_data = pl.read_csv(f\"{folder_rx}/{folder}/{file}\")\n",
    "                    data[spacing][osnr] = csv_file_data\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEkmEesdOkyQ"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KXXimWBpOQCH"
   },
   "outputs": [],
   "source": [
    "folder_rx = \"/home/jupyter-santiago.vargas5/DNN_Demodulation/data\"\n",
    "file_tx =\"/home/jupyter-santiago.vargas5/DNN_Demodulation/data/2x16QAM_16GBd.csv\"\n",
    "folder_json = \"/home/jupyter-santiago.vargas5/DNN_Demodulation/results\"\n",
    "# Datos transmitidos\n",
    "X_tx_norm = pl.read_csv(file_tx)\n",
    "X_tx_norm = np.array(X_tx_norm['I']) + np.array(X_tx_norm['Q'])*1j\n",
    "X_tx = sofa.mod_norm(X_tx_norm, 10)*X_tx_norm\n",
    "# Leer los datos recibidos\n",
    "data = read_data(folder_rx,'spacing')\n",
    "database = sofa.load_json(folder_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = \"/home/jupyter-santiago.vargas5/DNN_Demodulation/results\"\n",
    "# sofa.save_json(database, folder,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TYLfEFP43GY"
   },
   "source": [
    "# Calcular DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U6UWcUsELpI"
   },
   "source": [
    "[ ] Implementar opción para seleccionar modo:\n",
    "- Seleccionar más capas.\n",
    "- Seleccionar un espaciamiento en específico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisar Resultados y Posibles Errores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementos Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "-------------------\n",
      "-------------------\n",
      "-------------------\n",
      "-------------------\n",
      "{'15.5GHz': {'27dB': ['s8', 'rr32']}, '17GHz': {'25dB': ['rs32', 'rts32', 'rtt50', 'rst50']}}\n"
     ]
    }
   ],
   "source": [
    "def elementos_duplicados(diccionario):\n",
    "    valores_vistos = {}\n",
    "    duplicados = []\n",
    "\n",
    "    for clave, valor in diccionario.items():\n",
    "        if valor in valores_vistos:\n",
    "            #print(f'REPETIDO {clave}')\n",
    "            duplicados.append((clave, valor))\n",
    "        else:\n",
    "            valores_vistos[valor] = clave\n",
    "\n",
    "    return duplicados\n",
    "\n",
    "spacings = [15.5,16,16.5,17,17.6,18]\n",
    "dic_repetidos = {}\n",
    "for spacing in spacings:\n",
    "    \n",
    "    for osnr in database[f'{spacing}GHz']:\n",
    "        \n",
    "        if len(elementos_duplicados(database[f'{spacing}GHz'][osnr]))>0:\n",
    "            #print(spacing, osnr)\n",
    "            #print(elementos_duplicados(database[f'{spacing}GHz'][osnr]))\n",
    "            resultado = elementos_duplicados(database[f'{spacing}GHz'][osnr])\n",
    "            config_list = []\n",
    "            for elementos in resultado:\n",
    "                repet = list(database[f'{spacing}GHz'][osnr].keys())[list(database[f'{spacing}GHz'][osnr].values()).index(elementos[1])]\n",
    "                config_list.append(repet)\n",
    "                config_list.append(elementos[0])\n",
    "                \n",
    "                #print('Clave repetida: '+list(database[f'{spacing}GHz'][osnr].keys())[list(database[f'{spacing}GHz'][osnr].values()).index(elementos[1])])\n",
    "                print('-------------------')\n",
    "            dic_repetidos[f'{spacing}GHz'] = {}\n",
    "            dic_repetidos[f'{spacing}GHz'][osnr] =  config_list\n",
    "print(dic_repetidos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisar Escenarios Pendientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revisar 15.5GHz - 35dB - {'rr32'}\n",
      "Revisar 16GHz - 23dB - {'rr32'}\n",
      "Revisar 16GHz - 19dB - {'rr32'}\n",
      "Revisar 16GHz - 20dB - {'rr32'}\n",
      "Revisar 16GHz - 21.5dB - {'rr32'}\n",
      "Revisar 16GHz - 18dB - {'rr32'}\n",
      "Revisar 16GHz - 27dB - {'rr32'}\n",
      "Revisar 16GHz - 30dB - {'rr32'}\n",
      "Revisar 16GHz - 35dB - {'rr32'}\n",
      "Revisar 16GHz - 25dB - {'rr32'}\n",
      "Revisar 16GHz - 32dB - {'rr32'}\n",
      "Revisar 16GHz - 40dB - {'rr32'}\n",
      "Revisar 16.5GHz - 18dB - {'rr32'}\n",
      "Revisar 16.5GHz - 20dB - {'rr32'}\n",
      "Revisar 16.5GHz - 23dB - {'rr32'}\n",
      "Revisar 16.5GHz - 21.5dB - {'rr32'}\n",
      "Revisar 16.5GHz - 19dB - {'rr32'}\n",
      "Revisar 16.5GHz - 35dB - {'rr32'}\n",
      "Revisar 16.5GHz - 25dB - {'rr32'}\n",
      "Revisar 16.5GHz - 30dB - {'rr32'}\n",
      "Revisar 16.5GHz - 40dB - {'rr32'}\n",
      "Revisar 16.5GHz - 32dB - {'rr32'}\n",
      "Revisar 16.5GHz - 27dB - {'rr32'}\n",
      "Revisar 17GHz - 18dB - {'rr32'}\n",
      "Revisar 17GHz - 19dB - {'rr32'}\n",
      "Revisar 17GHz - 20dB - {'rr32'}\n",
      "Revisar 17GHz - 32dB - {'rr32'}\n",
      "Revisar 17GHz - 35dB - {'rr32'}\n",
      "Revisar 17GHz - 21.5dB - {'rr32'}\n",
      "Revisar 17GHz - 25dB - {'rr32'}\n",
      "Revisar 17GHz - 23dB - {'rr32'}\n",
      "Revisar 17GHz - 27dB - {'rr32'}\n",
      "Revisar 17GHz - 30dB - {'rr32'}\n",
      "Revisar 17GHz - 40dB - {'rr32'}\n",
      "Revisar 17.6GHz - 20dB - {'rr32'}\n",
      "Revisar 17.6GHz - 23dB - {'rr32'}\n",
      "Revisar 17.6GHz - 21.5dB - {'rr32'}\n",
      "Revisar 17.6GHz - 18dB - {'rr32'}\n",
      "Revisar 17.6GHz - 19dB - {'rr32'}\n",
      "Revisar 17.6GHz - 32dB - {'rr32'}\n",
      "Revisar 17.6GHz - 27dB - {'rr32'}\n",
      "Revisar 17.6GHz - 25dB - {'rr32'}\n",
      "Revisar 17.6GHz - 35dB - {'rr32'}\n",
      "Revisar 17.6GHz - 30dB - {'rr32'}\n",
      "Revisar 17.6GHz - 40dB - {'rr32'}\n",
      "Revisar 18GHz - 18dB - {'rr32'}\n",
      "Revisar 18GHz - 27dB - {'rr32'}\n",
      "Revisar 18GHz - 20dB - {'rr32'}\n",
      "Revisar 18GHz - 25dB - {'rr32'}\n",
      "Revisar 18GHz - 19dB - {'rr32'}\n",
      "Revisar 18GHz - 30dB - {'rr32'}\n",
      "Revisar 18GHz - 23dB - {'rr32'}\n",
      "Revisar 18GHz - 40dB - {'rr32'}\n",
      "Revisar 18GHz - 32dB - {'rr32'}\n",
      "Revisar 18GHz - 35dB - {'rr32'}\n"
     ]
    }
   ],
   "source": [
    "spacings = [15,15.5,16,16.5,17,17.6,18]\n",
    "conf_list = [\"rr32\"]\n",
    "for spacing in spacings:    \n",
    "    for osnr in database[f'{spacing}GHz']:\n",
    "        \n",
    "        if not(set(conf_list) <= set(list(database[f'{spacing}GHz'][osnr].keys()))):\n",
    "            resta = set(conf_list) - set(list(database[f'{spacing}GHz'][osnr].keys()))\n",
    "            print(f\"Revisar {spacing}GHz - {osnr} - {resta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar Elementos Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sp in dic_repetidos:\n",
    "#     for snr in dic_repetidos[sp]:\n",
    "#         for val_rep in dic_repetidos[sp][snr]:\n",
    "#             del database[sp][snr][val_rep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correr Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "editable": true,
    "id": "bdeVG0Tb45Tw",
    "outputId": "c7de69cf-dc47-4d4d-b316-6d921ea5b5d6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espaciamiento actual: 15GHz\n",
      "40dB\n",
      "=================Continuando 40dB=================\n",
      "rr\n",
      "Se han recorrido todas las neuronas\n",
      "30dB\n",
      "=================Continuando 30dB=================\n",
      "rr\n",
      "Se han recorrido todas las neuronas\n",
      "27dB\n",
      "=================Continuando 27dB=================\n",
      "rr\n",
      "Se han recorrido todas las neuronas\n",
      "32dB\n",
      "=================Continuando 32dB=================\n",
      "rr\n",
      "Se han recorrido todas las neuronas\n",
      "23dB\n",
      "=================Continuando 23dB=================\n",
      "rr\n",
      "Se han recorrido todas las neuronas\n",
      "25dB\n",
      "=================Continuando 25dB=================\n",
      "rr\n",
      "Se han recorrido todas las neuronas\n",
      "35dB\n",
      "=================Continuando 35dB=================\n",
      "rr\n",
      "Se han recorrido todas las neuronas\n",
      "Espaciamiento actual: 15.5GHz\n",
      "40dB\n",
      "=================Continuando 40dB=================\n",
      "rr\n",
      "Se han recorrido todas las neuronas\n",
      "30dB\n",
      "=================Continuando 30dB=================\n",
      "rr\n",
      "Se han recorrido todas las neuronas\n",
      "27dB\n",
      "=================Continuando 27dB=================\n",
      "rr\n",
      "Se han recorrido todas las neuronas\n",
      "21.5dB\n",
      "=================Continuando 21.5dB=================\n",
      "rr\n",
      "Se han recorrido todas las neuronas\n",
      "32dB\n",
      "=================Continuando 32dB=================\n"
     ]
    }
   ],
   "source": [
    "spacings = [15,15.5,16,16.5,17,17.6,18]\n",
    "for spacing in spacings:\n",
    "    X_rx = data[f'{spacing}GHz']\n",
    "    print(f\"Espaciamiento actual: {spacing}GHz\")\n",
    "    #demodulation(X_rx,X_tx, spacing,lista neuronas,[],4, [\"relu\",\"relu\", \"relu\"],database, False)\n",
    "    demodulation(X_rx=X_rx,\n",
    "                 X_tx=X_tx,\n",
    "                 spacing=spacing,\n",
    "                 lista_neuronas=[32],\n",
    "                 #Agregar OSNR específicos si se desea. (poner especific=True)\n",
    "                 OSNR=[32],\n",
    "                 n_splits=4,\n",
    "                 activations=[\"relu\", \"relu\"],\n",
    "                 database=database,\n",
    "                 #Indicar si se trabajan con valores de OSNR específico para evitar errores\n",
    "                 especific=False\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "editable": true,
    "id": "bdeVG0Tb45Tw",
    "outputId": "c7de69cf-dc47-4d4d-b316-6d921ea5b5d6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spacings = [15,15.5,16,16.5,17, 17.6,18]\n",
    "for spacing in spacings:\n",
    "    X_rx = data[f'{spacing}GHz']\n",
    "    print(f\"Espaciamiento actual: {spacing}GHz\")\n",
    "    #demodulation(X_rx,X_tx, spacing,lista neuronas,[],4, [\"relu\",\"relu\", \"relu\"],database, False)\n",
    "    demodulation(X_rx=X_rx,\n",
    "                 X_tx=X_tx,\n",
    "                 spacing=spacing,\n",
    "                 lista_neuronas=[32],\n",
    "                 #Agregar OSNR específicos si se desea. (poner especific=True)\n",
    "                 OSNR=[32],\n",
    "                 n_splits=4,\n",
    "                 activations=[\"relu\", \"relu\"],\n",
    "                 database=database,\n",
    "                 #Indicar si se trabajan con valores de OSNR específico para evitar errores\n",
    "                 especific=False\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinaciones faltantes:\n",
    "sigmoid 8 ---- DONE :D\n",
    "\n",
    "tanh 8 --- In progress\n",
    "\n",
    "relu 32, relu 16 --- In progress :D\n",
    "\n",
    "sigmoid 32, relu 16\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1uMeX68Zaxx-",
    "q_hIf5rLa3IP",
    "WEUURa50YqQ3"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
